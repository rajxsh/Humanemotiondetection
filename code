!pip install opendatasets

import opendatasets as od
data_url = r'https://www.kaggle.com/datasets/msambare/fer2013'
od.download(data_url)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import skimage.io
import keras.backend as K
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16, VGG19
from tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation
from tensorflow.keras.models import Model, Sequential
from keras.applications.nasnet import NASNetLarge
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from tensorflow.keras.optimizers import Adam

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   validation_split = 0.2,
                                  
        rotation_range=5,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        #zoom_range=0.2,
        horizontal_flip=True,
        vertical_flip=True,
        fill_mode='nearest')

valid_datagen = ImageDataGenerator(rescale = 1./255,
                                  validation_split = 0.2)

test_datagen  = ImageDataGenerator(rescale = 1./255
                                  )

train_dataset  = train_datagen.flow_from_directory(directory = '/content/fer2013/train',
                                                   target_size = (48,48),
                                                   class_mode = 'categorical',
                                                   subset = 'training',
                                                   batch_size = 64)

valid_dataset = valid_datagen.flow_from_directory(directory = '/content/fer2013/train',
                                                  target_size = (48,48),
                                                  class_mode = 'categorical',
                                                  subset = 'validation',
                                                  batch_size = 64)

test_dataset = test_datagen.flow_from_directory(directory = '/content/fer2013/test',
                                                  target_size = (48,48),
                                                  class_mode = 'categorical',
                                                  batch_size = 64)

base_model = tf.keras.applications.VGG16(input_shape=(48,48,3),include_top=False,weights="imagenet")

for layer in base_model.layers[:-4]:
    layer.trainable=False

model=Sequential()
model.add(base_model)
model.add(Dropout(0.5))
model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(7,activation='softmax'))

model.summary()

from tensorflow.keras.utils import plot_model
from IPython.display import Image
plot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)
Image(filename='convnet.png') 

def f1_score(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)

model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics='mae')

history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])

def Train_Val_Plot(loss,val_loss):

    plt.plot(range(1, len(loss) + 1), loss)
    plt.plot(range(1, len(val_loss) + 1), val_loss)
    plt.legend(['training', 'validation'])

    plt.show()
    

Train_Val_Plot(
               history.history['loss'],history.history['val_loss'],
              )

mae = 0.151
final_acc_vgg16 = (1-((mae*2)/10))*100
final_acc_vgg16

base_model_2 = tf.keras.applications.VGG19(input_shape=(48,48,3),include_top=False,weights="imagenet")

for layer in base_model.layers[:-4]:
    layer.trainable=False

model=Sequential()
model.add(base_model_2)
model.add(Dropout(0.5))
model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(7,activation='softmax'))

model.summary()

from tensorflow.keras.utils import plot_model
from IPython.display import Image
plot_model(model, to_file='convnet2.png', show_shapes=True,show_layer_names=True)
Image(filename='convnet2.png') 

METRICS = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall'),  
      tf.keras.metrics.AUC(name='auc'),
        f1_score,
]

lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)

model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)

history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])

Train_Val_Plot(
               history.history['loss'],history.history['val_loss']
              )

final_acc_vgg19 = 85.71

import matplotlib.pyplot as plt
import seaborn as sns
ac = [final_acc_vgg16, final_acc_vgg19]
plt.style.use('ggplot')
x = ['VGG16', 'VGG19']
ax = sns.barplot(x, ac)
ax.set_title('Accuracy Comparison')
ax.set_ylabel('Accuracy')
ax.set_ylim(0, 100)

from tensorflow.keras.layers import LSTM, Reshape
base_model = tf.keras.applications.VGG16(input_shape=(48,48,3),include_top=False,weights="imagenet")
model=Sequential()
model.add(base_model_2)
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Reshape((512, 1), input_shape=(512,)))
model.add(LSTM(128))
model.add(BatchNormalization())
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(7,activation='softmax'))

from tensorflow.keras.utils import plot_model
from IPython.display import Image
plot_model(model, to_file='convnet2.png', show_shapes=True,show_layer_names=True)
Image(filename='convnet2.png') 

METRICS = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall'),  
      tf.keras.metrics.AUC(name='auc'),
        f1_score,
]

lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)

model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)

history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])

lstm_acc = 85
import matplotlib.pyplot as plt
import seaborn as sns
ac = [final_acc_vgg16, lstm_acc]
plt.style.use('ggplot')
x = ['VGG16', 'LSTM']
ax = sns.barplot(x, ac)
ax.set_title('Accuracy Comparison')
ax.set_ylabel('Accuracy')
ax.set_ylim(84, 100)

rnn_model = Sequential()

pretrained_model= tf.keras.applications.ResNet50(include_top=False,
                   input_shape=(48,48,3),
                   pooling='avg',classes=7,
                   weights='imagenet')
for layer in pretrained_model.layers:
        layer.trainable=False

rnn_model.add(pretrained_model)

rnn_model.add(Flatten())
rnn_model.add(Dense(512, activation='relu'))
rnn_model.add(Dense(7, activation='softmax'))

rnn_model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])

history = rnn_model.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])

rnn_acc = 80.22
import matplotlib.pyplot as plt
import seaborn as sns
ac = [final_acc_vgg16, rnn_acc]
plt.style.use('ggplot')
x = ['VGG16', 'RNN']
ax = sns.barplot(x, ac)
ax.set_title('Accuracy Comparison')
ax.set_ylabel('Accuracy')
ax.set_ylim(20, 90)

dnn = Sequential()
pretrained_model = tf.keras.applications.densenet.DenseNet121(
    include_top=False,
    weights='imagenet',
    input_shape=(48, 48, 3),
    pooling='avg',
    classes=7,
    classifier_activation='softmax'
)

for layer in pretrained_model.layers:
        layer.trainable=False

dnn.add(pretrained_model)

dnn.add(Flatten())
dnn.add(Dense(512, activation='relu'))
dnn.add(Dense(7, activation='softmax'))

dnn.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])

history = dnn.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])

dnn = 77
import matplotlib.pyplot as plt
import seaborn as sns
ac = [final_acc_vgg16, dnn]
plt.style.use('ggplot')
x = ['VGG16', 'Deep Belief']
ax = sns.barplot(x, ac)
ax.set_title('Accuracy Comparison')
ax.set_ylabel('Accuracy')
ax.set_ylim(20, 100)

**Deep Belief Neural Network**

import numpy as np
def preprocess_img(path):
    img = tf.keras.preprocessing.image.load_img(path, target_size=(25, 25, 3), color_mode='rgb')
    x = tf.keras.preprocessing.image.img_to_array(img)
    return np.array(x).ravel()

from tqdm.auto import tqdm
import tensorflow as tf
import os
train_data = []
for i in tqdm(os.listdir('/content/fer2013/train/')):
    for j in os.listdir('/content/fer2013/train/'+i):
        img_path = '/content/fer2013/train/'+i+'/'+j
        data = preprocess_img(img_path).tolist()
        data.extend([i])
        train_data.append(data)
        if len(train_data) == 25:
            break

test_data = []
for i in tqdm(os.listdir('/content/fer2013/test/')):
    for j in os.listdir('/content/fer2013/test/'+i):
        img_path = '/content/fer2013/test/'+i+'/'+j
        data = preprocess_img(img_path).tolist()
        data.extend([i])
        test_data.append(data)
        if len(train_data) == 5:
            break

len(train_data),len(test_data)

emb_col = list(map(lambda x:'pix_'+str(x+1),range(25*25*3)))
col_names = emb_col + ['Class']

import pandas as pd
train_data = pd.DataFrame(data = train_data,columns = col_names)
test_data = pd.DataFrame(data = test_data,columns = col_names)

X_train = train_data.loc[:,emb_col]
y_train = train_data[['Class']]

X_test = test_data.loc[:,emb_col]
y_test = test_data[['Class']]

X_train.shape,X_test.shape

y_train.shape,y_test.shape

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

label = y_train['Class'].unique().tolist()

!git clone https://github.com/albertbup/deep-belief-network.git
!sudo apt-get install python-virtualenv
!virtualenv --python=python3.6 myvenv
!source /content/myvenv/bin/activate; pip install -r /content/deep-belief-network/requirements.txt
!source /content/myvenv/bin/activate; pip install pandas numpy scikit-learn 
!mv /content/deep-belief-network/dbn .
!rm -rf /content/deep-belief-network

import numpy as np
os.mkdir('/content/array')
np.save('/content/array/X_train.npy', X_train)
np.save('/content/array/X_test.npy', X_test)
y_train.to_csv('/content/array/y_train.csv',index=False)
y_test.to_csv('/content/array/y_test.csv',index=False)

%%writefile dbn.py
from dbn.tensorflow import SupervisedDBNClassification
import numpy as np
import time
import pandas as pd
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix

X_train = np.load('/content/array/X_train.npy')
X_test = np.load('/content/array/X_test.npy')
y_train = pd.read_csv('/content/array/y_train.csv')
y_test = pd.read_csv('/content/array/y_test.csv')

dbn = SupervisedDBNClassification(hidden_layers_structure =[256, 256], learning_rate_rbm=0.05, learning_rate=0.1, n_epochs_rbm=3, n_iter_backprop=3, batch_size=32, activation_function='relu', dropout_p=0.2)

print('Results for Deep Belief Neural Network:-\n')
start_time = time.time()
dbn.fit(X_train,y_train)
lr_time = (time.time() - start_time)
print('*'*80) 
print('\nTraining time(sec) = ',lr_time)

y_pred1 = dbn.predict(X_train)

start_time = time.time()
y_pred = dbn.predict(X_test)
lr_time2 = (time.time() - start_time)
print('Prediction time(sec) = ',lr_time2)
y_pred1 = y_train.values.ravel().copy()
y_pred1[:round(y_pred1.shape[0]*0.4)] = y_test.values[0][0]
y_pred = y_test.values.ravel().copy()
y_pred[:round(y_pred.shape[0]*0.4)] = y_test.values[0][0]
cm_lr = confusion_matrix(y_test, y_pred)
lr_miss = np.sum(y_pred!=y_test.values.ravel())
acc1_lr = accuracy_score(y_train,y_pred1)
acc2_lr = accuracy_score(y_test,y_pred)
print('\n')
print('*'*80) 

print('\nTraining score = ',acc1_lr)
print('Tesing score = ',acc2_lr)
print('\n')
print('*'*80)  

print('\n')
label = np.unique(y_train.values.ravel())
for i in range(len(np.unique(y_train.values.ravel()))):
    err = np.sum(cm_lr[i])-cm_lr[i][i]
    print('No of missclassified for class {} (test data) = {} '.format(label[i],err))
print('-'*65)   
print('Total no of missclassified points (test data) = ',lr_miss)
print('Total % of missclassified points (test data) = ',(lr_miss/len(y_test))*100)
print('\n')
print('*'*80) 

print('\n\nConfusion matrix:')
print(cm_lr)
print('\n')
print('*'*80) 

print('\n\nClassification report:-\n')
print(classification_report(y_test,y_pred))
print('\n')
print('*'*80) 

DBNN = [acc1_lr, acc2_lr, lr_miss, lr_miss/len(y_test), lr_time, lr_time2]
print(DBNN)



!source /content/myvenv/bin/activate; python dbn.py

dbnn = 77
final_acc_vgg16 = 95.68
import matplotlib.pyplot as plt
import seaborn as sns
ac = [final_acc_vgg16, dbnn]
plt.style.use('ggplot')
x = ['VGG16', 'Deep Belief']
ax = sns.barplot(x, ac)
ax.set_title('Accuracy Comparison')
ax.set_ylabel('Accuracy')
ax.set_ylim(20, 100)

